{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb        rfc_predictions.csv   test.csv\r\n",
      "f_rfr.csv             rfr_norm.csv          train.csv\r\n",
      "gender_submission.csv rfr_predictions.csv\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'rfr_norm.csv',\n",
       " 'Untitled.ipynb',\n",
       " 'rfc_predictions.csv',\n",
       " 'f_rfr.csv',\n",
       " 'test.csv',\n",
       " 'train.csv',\n",
       " '.ipynb_checkpoints',\n",
       " 'rfr_predictions.csv',\n",
       " 'gender_submission.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "#!conda install --yes --prefix {sys.prefix} scikit-learn\n",
    "!ls\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 12\n",
      "Test: Index(['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Parch_0',\n",
      "       'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6',\n",
      "       'Parch_9'],\n",
      "      dtype='object')\n",
      "Train: Index(['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Parch_0',\n",
      "       'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "features_tr = pd.get_dummies(train_data[['Pclass', 'Sex', 'Parch']], columns=['Pclass', 'Sex', 'Parch'])\n",
    "features_te = pd.get_dummies(test_data[['Pclass', 'Sex', 'Parch']], columns=['Pclass', 'Sex', 'Parch'])\n",
    "\n",
    "print(len(features_te.columns), len(features_tr.columns))\n",
    "# > Different Parch value for test\n",
    "\n",
    "print('Test:', features_te.columns)\n",
    "print('Train:', features_tr.columns)\n",
    "# Parch 9 must be deleted (Note: This makes our prediction worse, problem here is too small tr set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Index(['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Parch_0',\n",
      "       'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6'],\n",
      "      dtype='object')\n",
      "Train: Index(['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Parch_0',\n",
      "       'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Manually delete Parch\n",
    "features_te = features_te.drop(labels='Parch_9', axis=1)\n",
    "print('Test:', features_te.columns)\n",
    "print('Train:', features_tr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train RF Algorithm\n",
    "# Trying both RFclassifier & RFregressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "X = features_tr\n",
    "y = train_data['Survived']\n",
    "\n",
    "# RFC Model\n",
    "model_rfc = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features='sqrt')\n",
    "\n",
    "# Fit RFC\n",
    "model_rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = features_te\n",
    "rfc_predictions = model_rfc.predict(X_test)\n",
    "\n",
    "# Turn into df & Save as csv\n",
    "predictions_rfc = pd.DataFrame(data={'PassengerId': test_data['PassengerId'], 'Survived': rfc_predictions})\n",
    "predictions_rfc.to_csv('rfc_predictions.csv', index=False)\n",
    "predictions_rfc.head()\n",
    "# Score: 75.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out RF Regressor & Round values\n",
    "\n",
    "# Max_dept = 4 as only 3 parameters\n",
    "model_rfr = RandomForestRegressor(n_estimators=100, max_depth=4, max_features='sqrt')\n",
    "\n",
    "model_rfr.fit(X, y)\n",
    "\n",
    "rfr_predictions = model_rfr.predict(X_test).round().astype(int)\n",
    "print(rfr_predictions[:10])\n",
    "predictions_rfr = pd.DataFrame(data={'PassengerId': test_data['PassengerId'], 'Survived': rfr_predictions})\n",
    "predictions_rfr.to_csv('rfr_predictions.csv', index=False)\n",
    "predictions_rfr.head()\n",
    "# Score: 76.076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes: \n",
    "# I think sqrt sucks for the few features I take\n",
    "# Do I need dummies for RF? \n",
    "# Without OH-encoding\n",
    "\n",
    "f_tr = train_data[['Pclass', 'Sex', 'Parch']].copy()\n",
    "f_te = test_data[['Pclass', 'Sex', 'Parch']].copy()\n",
    "\n",
    "\n",
    "# Turn male into 1 & female into 0\n",
    "f_tr.loc[f_tr['Sex'] == 'male', 'Sex'] = 1\n",
    "f_tr.loc[f_tr['Sex'] == 'female', 'Sex'] = 0\n",
    "\n",
    "f_te.loc[f_te['Sex'] == 'male', 'Sex'] = 1\n",
    "f_te.loc[f_te['Sex'] == 'female', 'Sex'] = 0\n",
    "\n",
    "X = f_tr\n",
    "y = train_data['Survived']\n",
    "X_test = f_te\n",
    "\n",
    "mod_rfc = RandomForestRegressor(n_estimators=100, bootstrap=True, max_features='sqrt')\n",
    "mod_rfc.fit(X, y)\n",
    "\n",
    "pred_rfc = mod_rfc.predict(X_test).round().astype(int)\n",
    "\n",
    "fin_pred = pd.DataFrame(data={'PassengerId': test_data['PassengerId'], 'Survived': pred_rfc})\n",
    "fin_pred.to_csv('rfr_norm.csv', index=False)\n",
    "# Score: 77.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8426966292134832\n"
     ]
    }
   ],
   "source": [
    "# Back to starters; Splitting up the train set to be able to test it w/o wifi\n",
    "import copy\n",
    "\n",
    "# 891 entries in train_data\n",
    "split = round(0.9*891)\n",
    "\n",
    "t_set = train_data[:split].copy()\n",
    "t_sol = train_data[:split]['Survived'].copy()\n",
    "v_set = train_data[split:].copy()\n",
    "v_sol = train_data[split:]['Survived'].copy()\n",
    "\n",
    "# Full Training for submission\n",
    "f_set = train_data.copy()\n",
    "f_sol = train_data['Survived'].copy()\n",
    "s_set = test_data.copy()\n",
    "\n",
    "# Compute age average to replace NaN vaues wth it ; Median has ~same perf\n",
    "age_avg = train_data['Age'].mean().astype(int)\n",
    "\n",
    "# Encode Male/Female to 1 & 0 ; Encode Embarked to 2, 1, 0\n",
    "# Swapping out NaN values\n",
    "# Name & Ticket Nr should not make a difference\n",
    "\n",
    "# print(i.dtypes)\n",
    "# print(train_data.loc[train_data['Embarked'].isnull(), 'Embarked'])\n",
    "# print(train_data.loc[829]) \n",
    "\n",
    "for i in [t_set, v_set, f_set, s_set]:\n",
    "    i.loc[i['Sex'] == 'male', 'Sex'] = 1\n",
    "    i.loc[i['Sex'] == 'female', 'Sex'] = 0\n",
    "\n",
    "    i.loc[i['Embarked'] == 'C', 'Embarked'] = 2\n",
    "    i.loc[i['Embarked'] == 'Q', 'Embarked'] = 1\n",
    "    i.loc[i['Embarked'] == 'S', 'Embarked'] = 0\n",
    "\n",
    "    i.loc[i['Age'].isnull(), 'Age'] = age_avg  \n",
    "    i.loc[i['Embarked'].isnull(), 'Embarked'] = 0  \n",
    "    i.loc[i['Fare'].isnull(), 'Fare'] = 0  \n",
    "    \n",
    "    i['Fare'] = np.float32(i['Fare'])\n",
    "    i['Age'] = np.float32(i['Age'])\n",
    " \n",
    "    \n",
    "# Defining Regressor Model\n",
    "# Not limiting max_depth & max_features with sqrt seems better\n",
    "m_rfr = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=None, max_features=None)\n",
    "m_rfr.fit(t_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']], t_sol)\n",
    "# Test Regressor Model\n",
    "p_rfr = m_rfr.predict(v_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']]).round().astype(int)\n",
    "\n",
    "# Calculate Accuracy by getting the diff in an np array: -1/1 if wrong; 0 if correct\n",
    "diff = p_rfr - np.array(v_sol)\n",
    "acc = 1 - (sum(np.abs(diff)) / len(diff))\n",
    "print(acc)\n",
    "\n",
    "# Full model & sub\n",
    "#m_rfr.fit(f_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']], f_sol)\n",
    "#pf = m_rfr.predict(s_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']]).round().astype(int)\n",
    "#f_pred = pd.DataFrame(data={'PassengerId': test_data['PassengerId'], 'Survived': pf})\n",
    "#f_pred.to_csv('f_rfr.csv', index=False) # f for full referring to using the full spectrum of data\n",
    "\n",
    "# 76.55 Final Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a NN â€“ Do it once with torch, once self-coded\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # F functions\n",
    "        self.fc1 = nn.Linear(features, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=6, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_inputs = 6\n",
    "lr = 0.01\n",
    "betas = (0.9, 0.999)\n",
    "epochs = 100\n",
    "# Not using mini_batches, but full batches here, since not much data anyways\n",
    "\n",
    "\n",
    "m_nn = Network(features = n_inputs)\n",
    "m_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(m_nn.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_X, train_y, test_X, test_y, epochs=1):\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        \n",
    "        # Train\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        out = model(train_X)\n",
    "        \n",
    "        out = out.squeeze()\n",
    "        loss = (criterion(out, train_y))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Evaluate\n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "        out = model(test_X.float())\n",
    "        loss = criterion(out, test_y.float())\n",
    "\n",
    "        test_loss += loss.item()   \n",
    "        \n",
    "        # Accuracy\n",
    "        pred = np.round(out.detach().numpy())\n",
    "        test_y_np = test_y.numpy().reshape(-1, 1)\n",
    "        diff = np.abs(pred - test_y_np)\n",
    "        acc = 1 - (np.sum(diff) / len(diff))             \n",
    "        \n",
    "        if (i + 1) % 10 == 0: \n",
    "            print('Epoch: {}/{} \\t Train Loss: {} \\t Test Loss: {} \\t Accuracy: {}'.format(i+1, \n",
    "                epochs, round(train_loss, 5), round(test_loss, 5), round(acc, 5)))\n",
    "        \n",
    "    return model \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Epoch: 10/100 \t Train Loss: 0.25748 \t Test Loss: 0.24427 \t Accuracy: 0.62921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niklasmunnighoff/miniconda3/envs/titanic/lib/python3.7/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/niklasmunnighoff/miniconda3/envs/titanic/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([89])) that is different to the input size (torch.Size([89, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100 \t Train Loss: 0.24898 \t Test Loss: 0.24326 \t Accuracy: 0.62921\n",
      "Epoch: 30/100 \t Train Loss: 0.2338 \t Test Loss: 0.23498 \t Accuracy: 0.62921\n",
      "Epoch: 40/100 \t Train Loss: 0.21118 \t Test Loss: 0.27489 \t Accuracy: 0.70787\n",
      "Epoch: 50/100 \t Train Loss: 0.20539 \t Test Loss: 0.24553 \t Accuracy: 0.62921\n",
      "Epoch: 60/100 \t Train Loss: 0.19564 \t Test Loss: 0.26396 \t Accuracy: 0.62921\n",
      "Epoch: 70/100 \t Train Loss: 0.17526 \t Test Loss: 0.27648 \t Accuracy: 0.66292\n",
      "Epoch: 80/100 \t Train Loss: 0.17137 \t Test Loss: 0.29317 \t Accuracy: 0.73034\n",
      "Epoch: 90/100 \t Train Loss: 0.15843 \t Test Loss: 0.34133 \t Accuracy: 0.83146\n",
      "Epoch: 100/100 \t Train Loss: 0.17934 \t Test Loss: 0.29431 \t Accuracy: 0.7191\n"
     ]
    }
   ],
   "source": [
    "arr = np.array(t_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']])\n",
    "arr = np.float32(arr)\n",
    "train_X = torch.tensor(arr, dtype=torch.float)\n",
    "print(type(train_X))\n",
    "\n",
    "arr = np.array(t_sol)\n",
    "train_y = torch.tensor(arr, dtype=torch.float)\n",
    "\n",
    "arr = np.array(v_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']])\n",
    "arr = np.float32(arr)\n",
    "test_X = torch.tensor(arr, dtype=torch.float)\n",
    "\n",
    "arr = np.array(v_sol)\n",
    "test_y = torch.tensor(arr, dtype=torch.float)\n",
    "    \n",
    "tr_m = train(m_nn, criterion, optimizer, train_X, train_y, test_X, test_y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though performance is bad, predict & submit\n",
    "\n",
    "arr = np.array(s_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']])\n",
    "arr = np.float32(arr)\n",
    "s_test = torch.tensor(arr, dtype=torch.float)\n",
    "\n",
    "s_pred = tr_m(s_test)\n",
    "s_pred = np.round(s_pred.squeeze().detach().numpy()).astype(int)\n",
    "s_pred = pd.DataFrame(data={'PassengerId': test_data['PassengerId'], 'Survived': s_pred})\n",
    "s_pred.to_csv('nn.csv', index=False)\n",
    "\n",
    "# 0.70813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a NN from scratch\n",
    "\n",
    "def init_parameters(l_dims):\n",
    "    '''\n",
    "    Initializes parameters for NN.\n",
    "    \n",
    "    Args:\n",
    "    l_dims: features of each layer (incl. X layer)\n",
    "\n",
    "    Returns:\n",
    "    params: Dict w/ Weight & Bias matrices; Shapes: W: n[next] x n[previous] / B: n[next] x 1\n",
    "    '''\n",
    "    params = {}\n",
    "    \n",
    "    for i in range(1, len(l_dims)):\n",
    "        \n",
    "        params['W' + str(i)] = np.random.randn(l_dims[i], l_dims[i-1])\n",
    "        params['B' + str(i)] = np.random.randn(l_dims[i], 1)\n",
    "    \n",
    "    return params\n",
    "\n",
    "def L_model_forward(X, params):\n",
    "    '''\n",
    "    Propagates forward in network.\n",
    "    \n",
    "    Args:\n",
    "    X: Input matrix; Shape: nx (features) x m \n",
    "    parameters: Dict w/ W & B matrices; Shapes: W: n[next] x n[previous] / B: n[next] x 1\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat: Output; Shape: 1 x m\n",
    "    caches: Values needed for backprop\n",
    "    '''\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) // 2 # This is the amount of layers + 1 (X layer does not count!)\n",
    "    \n",
    "                                                          \n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = lin_forward(A_prev, params['W' + str(i)], params['B' + str(i)], 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    A_prev = A\n",
    "    A, cache = lin_forward(A_prev, params['W' + str(L)], params['B' + str(L)], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return A, caches\n",
    "        \n",
    "\n",
    "def lin_forward(A_prev, W, B, activation):\n",
    "    '''\n",
    "    Performs one step forward\n",
    "    \n",
    "    Args:\n",
    "    A_prev: Last activation; Shape: n_prev(==features), m\n",
    "    W: Weights Matrix for next layer; Shape: n[next] x n[previous]\n",
    "    B: Bias Matrix for next layer; Shape: n[next], 1\n",
    "    activation: String for activation we want to execute\n",
    "    \n",
    "    Returns:\n",
    "    A: Next activation; Shape: n[next] x m  \n",
    "    cache: dict w/ Z for later backprop; Z Shape: n[next] x m\n",
    "        (Note for A & Z n[next] are the n of that layer itself, hence they will turn to n_prev for the next layer)\n",
    "    '''\n",
    "    \n",
    "    Z = np.dot(W, A_prev) + B\n",
    "    linear_cache = (A_prev, W, B)\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        A = np.maximum(0, Z)  \n",
    "        \n",
    "    elif activation =='sigmoid':\n",
    "        A = 1/(1 - np.exp(-Z))\n",
    "        \n",
    "    elif activation == 'leaky_relu':\n",
    "        Z[Z<0] = 0.01 * Z[Z<0]\n",
    "        A = Z\n",
    "    \n",
    "    activation_cache = Z\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "    \n",
    "        \n",
    "def calc_loss(Y_hat, Y):\n",
    "    '''\n",
    "    Calculates loss of NN.\n",
    "    \n",
    "    Args:\n",
    "    Y_hat: Prediction; Shape: 1 x m\n",
    "    Y: Reality; Shape: 1 x m\n",
    "    \n",
    "    Returns:\n",
    "    L: L1 Loss; float\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    loss = 1/m * np.sum(0.5 * np.square(np.abs(Y_hat - Y)))\n",
    "    return loss\n",
    "    \n",
    "def L_model_backward(Y_hat, Y, caches):\n",
    "    '''\n",
    "    Backward propagates and calculates attributable losses via Deltas.\n",
    "    \n",
    "    Args:\n",
    "    Y_hat: The predictions of our model; Shape: 1 x m\n",
    "    Y: The reality; Shape: 1 x m\n",
    "    caches: Dictionary of stored values; Contains activation (Z) & linear caches (A_prev, W, B)\n",
    "    \n",
    "    Returns:\n",
    "    grads: Dictionary w/ dW & dB values; Shape: dW: n[next] x n[prev] / dB: n[next] x 1\n",
    "    '''\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Our L function was: Sum over m of |Y-hat - Y|\n",
    "    # dL/dZ = dL/dY_hat * dY_hat/dZ\n",
    "    dY_hat = 1/m * (Y_hat - Y)\n",
    "    cur_cache = caches[L-1] # This contains our Z\n",
    "    dA, dW, dB = lin_backward(dY_hat, cur_cache, 'sigmoid')\n",
    "    grads['dA' + str(L)], grads['dW' + str(L)], grads['dB' + str(L)] = dA, dW, dB\n",
    "    \n",
    "    # Loop from L-2 to 0; For 3 layers loop is: 1, 0; Hence +1 when storing\n",
    "    # We only loop over 2/3 layers, as one is done manually before\n",
    "    for i in reversed(range(L-1)):\n",
    "        cur_cache = caches[i]\n",
    "        dA_prev, dW, dB = lin_backward(grads['dA' + str(i+2)], cur_cache, 'leaky_relu')\n",
    "        grads['dA' + str(i+1)], grads['dW' + str(i+1)], grads['dB' + str(i+1)] = dA_prev, dW, dB\n",
    "        # We're storing the previous dA together with weights & biases of the next layer here\n",
    "        # i.e. the dA we store ets multiplied by the weights with it to arrive at the next Z's\n",
    "    return grads\n",
    "        \n",
    "def lin_backward(dA, cache, activation):\n",
    "    '''\n",
    "    Performs one step backward\n",
    "    \n",
    "    Args:\n",
    "    dA: The derivative of the last activation starting with dY_hat; Shape: n_prev(==features), m\n",
    "    cache: Contains the forward linear & activation parameters\n",
    "    activation: String describing the activation done on the forward step\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: The derivatve of the next A going backwards (This will be fed in again in lin_backward); Shape: n_prev, m\n",
    "    dW: The derivative of W in respect to our loss; Shape: n[next] x n [prev]\n",
    "    dB: The derivative of B in respect to our loss; Shape: n[next] x 1\n",
    "    '''\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    Z = activation_cache\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = np.array(dA, copy=True) \n",
    "        dZ[Z <= 0] = 0\n",
    "        dZ[Z > 0] = 1\n",
    "\n",
    "    elif activation =='sigmoid':\n",
    "        s = 1/(1 - np.exp(-Z))\n",
    "        dZ = dA * s * (1 - s) # Element wise multiplication, as we are multiplying the chain derivatives dL/dA * dA/dZ\n",
    "    \n",
    "    elif activation == 'leaky_relu':\n",
    "        dZ = np.array(dA, copy=True) \n",
    "        dZ[Z <= 0] = 0.01\n",
    "        dZ[Z > 0] = 1\n",
    "        \n",
    "        \n",
    "    A_prev, W, B = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    #print(dZ.shape, A_prev.shape)\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    dB = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, dB\n",
    "    \n",
    "    \n",
    "def update_parameters(params, grads, lr):\n",
    "    '''\n",
    "    Takes current cache & updates W & B\n",
    "    \n",
    "    Args:\n",
    "    params: Dict containing our current parameters; W & B\n",
    "    grads: Dict containing the grads; dW & dB \n",
    "    lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    params: Dict containing updated parameters; W & B\n",
    "    '''\n",
    "    L = len(params) // 2\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        #print(grads['dW' + str(l)].shape,  params['W' + str(l)].shape)\n",
    "        params['W' + str(l)] -= lr * grads['dW' + str(l)]\n",
    "        params['B' + str(l)] -= lr * grads['dB' + str(l)] \n",
    "        \n",
    "    return params\n",
    "\n",
    "def get_batch(X, Y, j, mb_size):\n",
    "    '''\n",
    "    Returns a mini batch given an mb size\n",
    "    \n",
    "    Args:\n",
    "    X: Inputs; Shape: nx x m\n",
    "    Y: Outputs; Shape: 1 x m\n",
    "    j: jth minibatch\n",
    "    '''\n",
    "    return X[:, j*mb_size:(j+1)*mb_size], Y[:, j*mb_size:(j+1)*mb_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, test_X, l_dims=[4, 2, 1], lr=0.001, epochs=10, print_cost=True, mb_size=32):\n",
    "    '''\n",
    "    Trains an L-layer model with ReLU's & a final sigmoid.\n",
    "    \n",
    "    Args:\n",
    "    X: Input features; shape: nx x m\n",
    "    Y: Outputs; shape: 1 x m\n",
    "    l_dims: Features of each layer (must include X layer features as first no)\n",
    "    lr: Learning rate\n",
    "    epochs: Epochs to train\n",
    "    \n",
    "    Returns:\n",
    "    model: Trained model\n",
    "    '''\n",
    "    costs = []\n",
    "    params = init_parameters(l_dims)\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for i in range(0, epochs+1):\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        for j in range(0, m // mb_size):\n",
    "            \n",
    "            X_batch, Y_batch = get_batch(X, Y, j, mb_size)\n",
    "        \n",
    "            Y_hat, caches = L_model_forward(X_batch, params)\n",
    "            loss = calc_loss(Y_hat, Y_batch)\n",
    "            grads = L_model_backward(Y_hat, Y_batch, caches)\n",
    "            params = update_parameters(params, grads, lr)\n",
    "            \n",
    "            running_loss += loss\n",
    "        \n",
    "        if print_cost and i % 10 == 0:\n",
    "            print('Epoch: {}/{} \\t Cost: {}'.format(i, epochs, running_loss))\n",
    "    \n",
    "    # Calculating Accuracy\n",
    "    Y_hat, _ = L_model_forward(X, params)\n",
    "    Y_hat = np.round(Y_hat)\n",
    "    diff = np.abs(Y_hat - Y)\n",
    "    acc = 1 - (np.sum(diff) / diff.shape[1])   \n",
    "    \n",
    "    print('Final Accuracy on Test Set: {}'.format(acc))\n",
    "    \n",
    "    Y_hat, _ = L_model_forward(test_X, params)\n",
    "    Y_hat = np.round(Y_hat)\n",
    "    Y_hat = np.squeeze(Y_hat, axis=0).astype(int)\n",
    "    \n",
    "    man_pred = pd.DataFrame(data={'PassengerId': test_data['PassengerId'], 'Survived': Y_hat})\n",
    "    man_pred.to_csv('man_nn.csv', index=False)\n",
    "    \n",
    "            \n",
    "    return params\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 891) (1, 891)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(f_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']], dtype=np.float32).T\n",
    "test_X = np.array(s_set[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']], dtype=np.float32).T\n",
    "\n",
    "train_y = np.array(f_sol, dtype=np.float32)\n",
    "train_y = np.expand_dims(train_y, axis=0)\n",
    "\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niklasmunnighoff/miniconda3/envs/titanic/lib/python3.7/site-packages/ipykernel_launcher.py:74: RuntimeWarning: overflow encountered in exp\n",
      "/Users/niklasmunnighoff/miniconda3/envs/titanic/lib/python3.7/site-packages/ipykernel_launcher.py:158: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500 \t Cost: 16.337861314228935\n",
      "Epoch: 10/500 \t Cost: 5.1970451853638835\n",
      "Epoch: 20/500 \t Cost: 5.192869813528542\n",
      "Epoch: 30/500 \t Cost: 5.193470582239442\n",
      "Epoch: 40/500 \t Cost: 5.192972419089142\n",
      "Epoch: 50/500 \t Cost: 5.190236808372639\n",
      "Epoch: 60/500 \t Cost: 5.1915024059846875\n",
      "Epoch: 70/500 \t Cost: 5.192670800688336\n",
      "Epoch: 80/500 \t Cost: 5.189104208930733\n",
      "Epoch: 90/500 \t Cost: 5.187500028428315\n",
      "Epoch: 100/500 \t Cost: 5.1875\n",
      "Epoch: 110/500 \t Cost: 5.1875\n",
      "Epoch: 120/500 \t Cost: 5.187500000005828\n",
      "Epoch: 130/500 \t Cost: 5.1875\n",
      "Epoch: 140/500 \t Cost: 5.194779834170646\n",
      "Epoch: 150/500 \t Cost: 5.193837904211741\n",
      "Epoch: 160/500 \t Cost: 5.193514068058348\n",
      "Epoch: 170/500 \t Cost: 5.193514056725555\n",
      "Epoch: 180/500 \t Cost: 5.193514045392809\n",
      "Epoch: 190/500 \t Cost: 5.193514034060105\n",
      "Epoch: 200/500 \t Cost: 5.193514022727444\n",
      "Epoch: 210/500 \t Cost: 5.193514011394824\n",
      "Epoch: 220/500 \t Cost: 5.193514000062247\n",
      "Epoch: 230/500 \t Cost: 5.193513988729714\n",
      "Epoch: 240/500 \t Cost: 5.193513977398016\n",
      "Epoch: 250/500 \t Cost: 5.193513997708633\n",
      "Epoch: 260/500 \t Cost: 5.19351398637599\n",
      "Epoch: 270/500 \t Cost: 5.193513975043388\n",
      "Epoch: 280/500 \t Cost: 5.1935139637108305\n",
      "Epoch: 290/500 \t Cost: 5.193513952378315\n",
      "Epoch: 300/500 \t Cost: 5.193513941045842\n",
      "Epoch: 310/500 \t Cost: 5.193513929713412\n",
      "Epoch: 320/500 \t Cost: 5.1935139183810275\n",
      "Epoch: 330/500 \t Cost: 5.193513907048682\n",
      "Epoch: 340/500 \t Cost: 5.1935138957163804\n",
      "Epoch: 350/500 \t Cost: 5.193513884384121\n",
      "Epoch: 360/500 \t Cost: 5.193513873051907\n",
      "Epoch: 370/500 \t Cost: 5.19351386178506\n",
      "Epoch: 380/500 \t Cost: 5.193513854170632\n",
      "Epoch: 390/500 \t Cost: 5.19351380741331\n",
      "Epoch: 400/500 \t Cost: 5.1935137960815\n",
      "Epoch: 410/500 \t Cost: 5.193513784749736\n",
      "Epoch: 420/500 \t Cost: 5.193513773418015\n",
      "Epoch: 430/500 \t Cost: 5.19351376208634\n",
      "Epoch: 440/500 \t Cost: 5.193513750754703\n",
      "Epoch: 450/500 \t Cost: 5.193513739423111\n",
      "Epoch: 460/500 \t Cost: 5.19351372809156\n",
      "Epoch: 470/500 \t Cost: 5.193513716760054\n",
      "Epoch: 480/500 \t Cost: 5.193513705428589\n",
      "Epoch: 490/500 \t Cost: 5.193513694097199\n",
      "Epoch: 500/500 \t Cost: 5.193513682765867\n",
      "Final Accuracy on Test Set: 0.6161616161616161\n"
     ]
    }
   ],
   "source": [
    "X = train_X\n",
    "Y = train_y\n",
    "\n",
    "# Hyperparams\n",
    "epochs = 500\n",
    "lr = 0.001\n",
    "l_dims = [train_X.shape[0], 256, 128, 64, 1]\n",
    "\n",
    "params_t = L_layer_model(X, Y, test_X, l_dims, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Logbook [Change ; Results]\n",
    "\n",
    "# MiniBatches ; 100 Epochs: ~532.1 Cost\n",
    "# dZ[>0] ; 100 Epochs: ~536 cost, but steadier dec.\n",
    "# LeakyRELU ; 100 Epochs: 537.5 cost; 1000: 532.6\n",
    "# From L1 to L2 Loss (Y_H - Y & 1 to 0.5 * (Y_H - Y)^2 & )\n",
    "\n",
    "# Accuracy always the exact same\n",
    "# > It seems like the LR was to high as tiny changes in data matter or not\n",
    "# It seems like there is a maximum at Cost 532, which the algo is not able to surpass\n",
    "\n",
    "\n",
    "# maybe the sigmoid?  Too high & to low vals? replace with linear layer ?\n",
    "# All values seem to have the same decimals after comma \n",
    "# Weights seem to be exploding very quickly to high minus values\n",
    "\n",
    "\n",
    "# References:\n",
    "# https://www.kaggle.com/ktattan/3-layer-neural-network-on-titanic-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
